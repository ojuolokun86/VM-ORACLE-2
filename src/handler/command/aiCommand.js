const axios = require('axios');
const sendToChat = require('../../utils/sendToChat');

const AI_PROVIDERS = {
    'gpt': {
        name: "ğŸ¤– GPT-3.5",
        url: (query) => `https://lance-frank-asta.onrender.com/api/gpt?q=${encodeURIComponent(query)}`,
        method: 'GET',
        responseParser: (data) => {
            if (data && data.result) return data.result;
            if (data && data.message) return data.message;
            if (data && data.response) return data.response;
            return "No response from GPT-3.5";
        },
        errorMessage: "GPT-3.5 is currently unavailable."
    },
    'llama': {
        name: "ğŸ¦™ Meta Llama",
        url: (query) => `https://api.giftedtech.co.ke/api/ai/meta-llama?apikey=gifted&q=${encodeURIComponent(query)}`,
        method: 'GET',
        responseParser: (data) => {
            if (data && data.result) return data.result;
            if (data && data.answer) return data.answer;
            if (data && data.response) return data.response;
            return "No response from Llama";
        },
        errorMessage: "Llama AI is currently unavailable."
    },
    'mistral': {
        name: "ğŸŒ¬ï¸ Mistral AI",
        url: (query) => `https://api.giftedtech.co.ke/api/ai/mistral?apikey=gifted&q=${encodeURIComponent(query)}`,
        method: 'GET',
        responseParser: (data) => {
            if (data && data.result) return data.result;
            if (data && data.answer) return data.answer;
            if (data && data.response) return data.response;
            return "No response from Mistral";
        },
        errorMessage: "Mistral AI is currently unavailable."
    },
   'deepseek': {
            name: "ğŸ” DeepSeek V3",
            url: (query) => `https://api.giftedtech.co.ke/api/ai/deepseek-v3?apikey=gifted&q=${encodeURIComponent(query)}`,
            method: 'GET',
            responseParser: (data) => {
                // Handle case when API returns success but no result
                if (data && data.status === 200 && data.success) {
                    return "I'm sorry, I couldn't generate a response. Please try again or ask a different question.";
                }
                if (data && data.result) return data.result;
                if (data && data.answer) return data.answer;
                if (data && data.response) return data.response;
                return "No response from DeepSeek";
            },
            errorMessage: "DeepSeek V3 is currently unavailable."
        }
};

function formatAIResponse(provider, response) {
    const header = `â•­â”€â”€â”€  *${provider.name}*  â”€â”€â”€â•®\n\n`;
    const footer = `\n\nâ•°â”€â”€â”€  *BMM AI System*  â”€â”€â”€â•¯`;
    const formattedResponse = response
        .split('\n')
        .map(line => `â”‚ ${line}`)
        .join('\n');
    
    return header + formattedResponse + footer;
}

async function getAIResponse(provider, query) {
    try {
        const config = {
            timeout: 30000,
            validateStatus: function (status) {
                return status >= 200 && status < 500;
            }
        };

        const url = typeof provider.url === 'function' ? provider.url(query) : provider.url;
       // console.log(`[AI] Making ${provider.method} request to:`, url);
        
        let response;
        if (provider.method === 'POST') {
            const requestData = provider.getData ? provider.getData(query) : { query };
            //console.log('[AI] Request data:', requestData);
            response = await axios.post(url, requestData, config);
        } else {
            response = await axios.get(url, config);
        }
        

       // console.log(`[AI] Response status: ${response.status}`);
       // console.log('[AI] Response data:', JSON.stringify(response.data, null, 2));

        if (response.status !== 200) {
            throw new Error(`API returned status ${response.status}`);
        }

        const result = provider.responseParser(response.data);
        if (!result) {
            throw new Error('Empty or invalid response from AI provider');
        }
        return result;
    } catch (error) {
        console.error(`Error with ${provider.name}:`, error.message);
        if (error.response) {
            console.error('Response error data:', error.response.data);
        }
        throw error;
    }
}

async function aiCommand(sock, chatId, msg, { prefix, args, command: cmd }) {
    const query = args.join(' ').trim();
    
    if (!query) {
        const helpMessage = `
â•­â”€â”€â”€  *AI COMMAND CENTER*  â”€â”€â”€â•®

â”‚ *Available Commands:*
â”‚
â”‚ â€¢ *${prefix}ai <message>*
â”‚   â””â”€ Use any available AI model
â”‚
â”‚ â€¢ *${prefix}gpt <message>*
â”‚   â””â”€ Chat with GPT-3.5
â”‚
â”‚ â€¢ *${prefix}llama <message>*
â”‚   â””â”€ Chat with Meta Llama
â”‚
â”‚ â€¢ *${prefix}mistral <message>*
â”‚   â””â”€ Chat with Mistral AI
â”‚
â”‚ â€¢ *${prefix}deepseek <message>*
â”‚   â””â”€ Chat with DeepSeek V3
â”‚
â”‚ Example: *${prefix}ai* How does quantum computing work?

â•°â”€â”€â”€  *BMM AI System*  â”€â”€â”€â•¯
        `;
        
        return sendToChat(sock, chatId, { message: helpMessage });
    }

    try {
        await sendToChat(sock, chatId, {
            message: "â³ *Processing your request...*"
        });

        let providersToTry = [];
        
        if (cmd === 'gpt') {
            providersToTry = [AI_PROVIDERS.gpt, AI_PROVIDERS.llama, AI_PROVIDERS.mistral, AI_PROVIDERS.deepseek];
        } else if (cmd === 'llama') {
            providersToTry = [AI_PROVIDERS.llama, AI_PROVIDERS.gpt, AI_PROVIDERS.mistral];
        } else if (cmd === 'mistral') {
            providersToTry = [AI_PROVIDERS.mistral, AI_PROVIDERS.gpt, AI_PROVIDERS.llama];
        } else if (cmd === 'deepseek' || cmd === 'ds') {
            providersToTry = [AI_PROVIDERS.deepseek, AI_PROVIDERS.gpt, AI_PROVIDERS.mistral];
        } else {
            providersToTry = Object.values(AI_PROVIDERS).sort(() => Math.random() - 0.5);
        }

        for (const [index, currentProvider] of providersToTry.entries()) {
            try {
                const aiResponse = await getAIResponse(currentProvider, query);
                if (aiResponse) {
                    const formattedResponse = formatAIResponse(currentProvider, aiResponse);
                    return sendToChat(sock, chatId, {
                        message: formattedResponse
                    });
                }
            } catch (error) {
                console.error(`Attempt ${index + 1} with ${currentProvider.name} failed:`, error.message);
                
                if (index < providersToTry.length - 1) {
                    await sendToChat(sock, chatId, {
                        message: `âš ï¸ ${currentProvider.errorMessage} Trying next available AI...`
                    });
                    await new Promise(resolve => setTimeout(resolve, 1500));
                }
            }
        }

        throw new Error('All AI providers are currently unavailable.');

    } catch (error) {
        console.error('AI command error:', error);
        const errorMessage = `
â•­â”€â”€â”€  *AI SYSTEM ERROR*  â”€â”€â”€â•®

â”‚ âŒ Unable to get a response from any AI service.
â”‚
â”‚ Possible reasons:
â”‚ â€¢ High traffic on AI servers
â”‚ â€¢ Temporary service issues
â”‚ â€¢ Network connectivity problems
â”‚
â”‚ Please try again in a few minutes.

â•°â”€â”€â”€  *BMM AI System*  â”€â”€â”€â•¯
        `;
        return sendToChat(sock, chatId, { message: errorMessage });
    }
}

module.exports = aiCommand;